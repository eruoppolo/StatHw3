---
title: "GroupK_HM3"
author: "L.Pernice, E.Ruoppolo, M.Tallone, A.Valentinis"
date: "2022-11-20"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 6

### Ex 6.12

*For the ``UN`` data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.*

**Solution**

### Ex 6.14

*The data set ``Crabs2`` at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, SpermTotal, is the log of the total number of sperm in an ejaculate. It has $\bar{y}= 19.3$ and $s = 2.0$. The two explanatory variables used in the `R` output are the horseshoe crab’s carapace width (CW, mean $18.6\,cm$, standard deviation $3.0\,cm$), which is a measure of its size, and color (1=dark, 2=medium, 3=light), which is a measure of adult age, darker ones being older.\
```{r}
Crabs2=read.table("data/Crabs2.dat", header = T)
summary(lm(SpermTotal ~ CW + factor(Color), data=Crabs2))
library(car)
Anova(lm(SpermTotal ~ CW + factor(Color) + CW:factor(Color), data=Crabs2))
```

(a) Using the results shown, write the prediction equation and interpret the parameter esti-
mates.\
(b) Explain the differences in what is tested with the F statistic (i) for the overall model, (ii)
for the factor(Color) effect, (iii) for the interaction term. Interpret each.*

**Solution**

### Ex 6.30

*When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat{\beta}_1$ in the bivariate linear model are also then multiplied by $c$. Thus, show that $r =\hat{\beta}_1(s_x/s_y)$ does not depend on the units of measurement.*

**Solution**

For simplicity we can consider two separate cases, for $c>0$ and $c <0$. Let's first consider the case $c>0$.
Let's call $z_i = c y_i$ for the sake of clarity.
We can, then calculate $s_z = s_{cy}$
\[
\begin{split}
s_z &= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (z_i - \bar z)^2}= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (cy_i - \bar {cy_i})^2}=\\
&= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (cy_i - c\bar {y_i})^2}= \sqrt{\frac{c^2}{n-1} \sum_{i=1}^n (y_i - \bar {y_i})^2}=\\
&= c\sqrt{\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar {y_i})^2}= c s_y
\end{split}
\]

Let's now consider $\hat{\beta_1}'$ as the *slope* coefficient related to the response variables $z_i$.
\[
\begin{split}
\hat{\beta_1}' &= \frac{\sum_i (x_i -\bar x)(z_i -\bar z)}{\sum_i(x_i-\hat x)^2} = \frac{\sum_i (x_i -\bar x)(cy_i -\bar {cy_i})}{\sum_i(x_i-\hat x)^2} =\\
&= c \frac{\sum_i (x_i -\bar x)(y_i -\bar {y_i})}{\sum_i(x_i-\hat x)^2} = c \hat{\beta_1}
\end{split}
\]

Given these results, we can state that the quantity $r=\hat{\beta_1}\left(\frac{s_x}{s_y}\right)$ doesn't depend on the units of measurement, as
\[
r_z = \hat{\beta_1}' \frac{s_x}{s_z} = c \hat{\beta_1} \frac{s_x}{c s_y} = \hat{\beta_1} \frac{s_x}{s_y} = r_y
\]

Now, if $c<0$, we obtain, doing the same calculations, that $$s_z = |c| s_y$$, and, at the same time $$\hat\beta_1' = c \hat\beta_1$$.

So, When $c<0$, both $s_y$ and $\hat\beta_1$ preserve their magnitude, but the negative sign of $c$ leads to a negative scaling of the slope coefficient.
Despite this, the coefficient $r=\hat\beta_1 \left(\frac{s_x}{s_y}\right)$ remains unchanged.

### Ex 6.42

*You can fit the quadratic equation $\text{E}(Y)=\beta_0+\beta_1x+\beta_2x^2$ by fitting a multiple regression model with $x_1=x$ and $x_2=x^2$*.

(a) *Simulate 100 independent observations from the model $Y=40.0-5.0x+0.5x^2+\epsilon$, where $X$ has a uniform distribution over $[0,10]$ and $\epsilon\sim N(0,1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.*

(b) *Find the correlation between $x$ and $y$ and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.*

**Solution**

### Ex 6.52

*$F$ statistics have alternate expressions in terms of $R^2$ values.*

(a)   *Show that for testing $H_0:\,\beta_1=\dots=\beta_p=0$,*$$F=\frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} \,\text{ is equivalently }\,\frac{R^2/p}{(1-R^2)/[n-(p+1)]}$$ *Explain why larger values of $R^2$ yield larger values of $F$.*

(b) *Show that for comparing nested linear models,* $$ F=\frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} =\frac{(R_1^2-R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]}$$

**Solution**

## FSDS - Chapter 7

### Ex 7.4

*Analogously to the previous exercise, randomly sample 30 X observations from a uniform in the interval $(-4,4)$ and conditional on $X= x$, 30 normal observations with $\text{E}(Y)=3.5x^3-20x^2+0.5x+20$ and $\sigma = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $\text{E}(Y)=0.5x^3- 20x^2+0.5x+20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?*

**Solution**

### Ex 7.20

*In the `Crabs` data file introduced in Section 7.4.2, the variable $y$ indicates whether a female horseshoe crab has at least one satellite (1= yes, 0= no).*

(a) *Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.*

(b) *Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.*

(c) *Use AIC to determine which models seem most sensible among the models with*

    (i) interaction,

    (ii) main effects,

    (iii) weight as the sole predictor,

    (iv) color as the sole predictor,

    (v) the null model.

**Solution**

(a)

```{r}
crabs = read.table("data/Crabs.dat", header=T)
main_model = glm(y ~ weight + factor(color), data=crabs, family=binomial)
summary(main_model)
```
In this case we can say that none of the colors in the dataset show a statistical significance at a typical significance level ($\alpha = 0.05$). In fact, as their p-value is greater than this significance level, this shows weak evidence against the null hypothesis of $\hat\beta_j = 0$. So, we can say that there is insufficient evidence to conclude that any level of the covariate *color* affects the response variable $y$.

```{r}
# Confidence interval for weight effect
coef_weight <- coef(main_model)["weight"]
se_weight <- summary(main_model)$coefficients["weight", "Std. Error"]
z_value <- qnorm(0.975)
ci_lower <- coef_weight - z_value * se_weight
ci_upper <- coef_weight + z_value * se_weight
cat("Confidence Interval for the effect of weight: (", ci_lower, ",", ci_upper, ")\n")
```

(b)
```{r}
# Fit the model with interaction
interaction_model <- glm(y ~ weight * factor(color), data = crabs, family = binomial)
summary(interaction_model)

# Compare models using likelihood ratio test
anova(main_model, interaction_model, test = "Chisq")

```
As we can see, although looking at the only AIC value would suggest taking the model with interaction as a better one, looking at the p-value of the chi-squared statistics calculated upon the deviances, we can see that we don't have a significant improvement in the model, even considering the interaction factor. This suggests us to stick with the simpler model without interaction.

(c)
```{r}
#Model with interaction
interaction_model <- glm(y ~ weight * factor(color), data = crabs, family = binomial)
AIC_interaction <- AIC(interaction_model)

#Model without interaction
main_model <- glm(y ~ weight + factor(color), data = crabs, family = binomial)
AIC_main <- AIC(main_model)

#Model with only weight
weight_model <- glm(y ~ weight, data = crabs, family = binomial)
AIC_weight <- AIC(weight_model)

#Model with only color
color_model <- glm(y ~ factor(color), data = crabs, family = binomial)
AIC_color <- AIC(color_model)

#Null model
null_model <- glm(y ~ 1, data = crabs, family = binomial)
AIC_null <- AIC(null_model)

#AIC comperison
AIC_values <- c("Interaction" = AIC_interaction, 
                "Main Effects" = AIC_main, 
                "Weight Only" = AIC_weight, 
                "Color Only" = AIC_color, 
                "Null Model" = AIC_null)

AIC_values

```
Looking at only the AIC value, the model that seems best fit on the data is the one considering the interaction between *weight* and *color* covariates. So it seems to be the best balance between goodness of fit and complexity.

The second one in this list is the one that considers the simple interaction between *weight* and *color*, that has a slightly lower AIC index. This suggests that considering *weight* and *color* ad main effect without considering their interaction provides a reasonably good fit, too, while being less complex.

Also the model considering the *wieght* parameter as only covariate seems to have the same explanation.

The remaining two models have higher AIC, and considering they are simpler models, they suggest poorer fit of the data.

So considering the AIC index, the most sensible choice appears to be the one including the interaction between covariates, closely followed by the model without interaction.

### Ex 7.26

*A headline in ``The Gainesville Sun (Feb. 17, 2014)`` proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.*

**Solution**

## DAAG - Chapter 8

### Ex 6

*As in the previous exercise, the function poissonsim() allows for experimentation with
Poisson regression. In particular, poissonsim() can be used to simulate Poisson responses
with log-rates equal to a + bx, where a and b are fixed values by default.\
(a) Simulate 100 Poisson responses using the model $$log \lambda = 2 − 4x$$\
for x = 0, 0.01, 0.02 ..., 1.0. Fit a Poisson regression model to these data, and compare the
estimated coefficients with the true coefficients. How well does the estimated model predict
future observations?\
(b) Simulate 100 Poisson responses using the model $$log \lambda = 2 − bx$$
where b is normally distributed with mean 4 and standard deviation 5. [Use the argument
slope.sd=5 in the poissonsim() function] How do the results using the poisson
and quasipoisson families differ?*


**Solution**
