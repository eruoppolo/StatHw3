---
title: "GroupK_HM3"
author: "L.Pernice, E.Ruoppolo, M.Tallone, A.Valentinis"
date: "2022-11-20"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 6

### Ex 6.12

*For the ``UN`` data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.*

**Solution**

Fist of all, we can load the necessary dataset from the book's website with the packages later used in the analysis.

```{r, include=FALSE}
library(ellipse)
library(PerformanceAnalytics)
library(car)
un <- read.table("https://stat4ds.rwth-aachen.de/data/UN.dat", header=TRUE)
```

Then it's good practice to explore the data with the ``head()`` and the ``summary()`` functions.

```{r}
head(un)
```

```{r}
summary(un)
```

It's now possible to see that the "UN" dataset contains information on 42 different states. The objective is to build a model capable of predicting the response variable ``Internet``, which represents the percentage of people using internet in a given nation, using the other variables as covariates.\
Here is a brief description of the other variables present in the dataset:\

- ``Nation``: a list containing the names of the nations where the data were collected

- ``GDP``: per capita gross domestic product (in thousands of dollars)\

- ``HDI``: it's a human development index, which has components referring to life expectancy at birth, educational attainment, and income per capita.\

- ``GII``: a gender inequality index, a composite measure reflecting inequality in achievement between women and men in reproductive health, empowerment, and the labor market.\

- ``Fertility``: fertility rate (number of births per woman)\

- ``CO2``: carbon dioxide emissions per capita (in metric tons)\

- ``Homicide``: a homicide rate (number of homicides per 100,000 people)\

- ``Prison``: prison population (per 100,000 people)\

The first thing to notice is that the categorical variable ``Nation`` is not relevant for our model so we can drop it in order to conduct the analysis.
At the same time, for clarity purposes, we can assign meaningful names to the other columns.

```{r}
un <- subset(un, select = -c(Nation))
colnames(un) <- c("GDP", "HDI", "GII", "Fertility", "CO2", "Homicide", "Prison", "Internet")
```

As suggested from the exercise instruction, we can then try to build a multiple regression model to predict ``Internet`` using all the remaining as covariates. In other words, we need to estimate the coefficients $\beta_0, \beta_1, \dots, \beta_7$ for which:\
$$
(Internet)_i = \beta_0 + \beta_{1} \cdot (GDP)_i + \beta_{2} \cdot (HDI)_i + \beta_{3} \cdot (GII)_i + \beta_{4} \cdot (Fertility)_i + \beta_{5} \cdot (CO2)_i + \beta_{6} \cdot (Homicide)_i + \beta_{7} \cdot (Prison)_i + \epsilon_i
$$

Where $\epsilon_i \sim N(0, \sigma^2), \forall i=1,\dots,42$.\
This can be easily doe using the ``lm()`` function in R. We can build an object ``m2`` that represents our model and inspect its characteristics using the ``summary()`` function.
```{r}
m2 <- lm(Internet ~ ., data = un)
summary(m2)
```

From this first analysis we can quickly notice that, even if the adjusted $R^2=0.8164$, the estimated standard error for some of the parameters is very high, which is a sign of possible correlation between variables.\
To verify his we can visualize the scatterplot matrix for this dataset:

```{r}
chart.Correlation(un)
```

From the obtained plot we can indeed conclude that there is a high correlation between the first 3 variables and the response variable we're trying to predict. This correlation might be the possible source of uncertainty in the previous model.\
We can further investigate on this by computing the VIF in the following way:
```{r}
vif(m2)
```

The outcome of this computation seems to confirm out previous observation. Even if the VIF of the variables ``GDP`` and ``GII`` does not reach the usual threshold $10$ as the one for ``HDI`` does, it is still a sign of multicollinearity between the covariates.

The other thing we can notice is that the variables ``Homicide``, ``Prison`` and (to some extent) ``Fertility`` are not strictly correlated to the response variable.\
As the exercise suggest we can therefore repeat the fitting process using only the variable ``GDP`` as covariate.

```{r}
m1 <- lm(Internet ~ GDP, data = un)
summary(m1)
```

The resultant $R^2=0.7637$ of this fit is not far from the one obtained in the previous model. This is a consequence of the high correlation we've previously discussed. In fact, by definition, the adjusted $R^2$ penalizes the introduction of variables that are not significant in further explaining the model, which in this case are the variables ``HDI`` and ``GII`` due to their strong correlation to the ``GDP`` one.\
Moreover, it is possible to notice is that the $\hat\beta_1$ coefficient for the ``GDP`` variable estimated in the second model (``m1``) is greater that the one found in the first one. This is most probably another result of the multicollinearity problem related to the fact the the variables which are highly correlated with the ``GDP`` tend to "mask" the real effect of the latter in the full model (``m2``).

### Ex 6.14

*The data set ``Crabs2`` at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, SpermTotal, is the log of the total number of sperm in an ejaculate. It has $\bar{y}= 19.3$ and $s = 2.0$. The two explanatory variables used in the `R` output are the horseshoe crab’s carapace width (CW, mean $18.6\,cm$, standard deviation $3.0\,cm$), which is a measure of its size, and color (1=dark, 2=medium, 3=light), which is a measure of adult age, darker ones being older.\
```{r}
Crabs2=read.table("data/Crabs2.dat", header = T)
summary(lm(SpermTotal ~ CW + factor(Color), data=Crabs2))
library(car)
Anova(lm(SpermTotal ~ CW + factor(Color) + CW:factor(Color), data=Crabs2))
```

(a) Using the results shown, write the prediction equation and interpret the parameter esti-
mates.\
(b) Explain the differences in what is tested with the F statistic (i) for the overall model, (ii)
for the factor(Color) effect, (iii) for the interaction term. Interpret each.*

**Solution**


a.    Using the summary results we can write the equation:
$$ E(Y_i)=11.36+0.39\times CW_i+0.81\times C2_{i}+1.15\times C3_{i}$$
where $C2_{i}=\text{Color } 2\,(1=\text{yes},0=\text{no})$, $C3_{i}=\text{Color } 3\,(1=\text{yes},0=\text{no})$. 
In this way we categorized the three different colors fitting three lines, each for a different color of the crab. Each line intercept changes with respect to the color, while the slope is the same. 
The coefficient $\hat{\beta_2}=0.81$ then indicates the difference between the estimated mean value of $SpermTotal$ for Color2 and Color1 crabs, adjusting for carapace width. The same in the case of $\hat{\beta}_3$ but for Color3 and Color1 crabs. Finally for Color3 and Color2 this difference is $\hat{\beta}_3-\hat{\beta}_2=0.34$.

To visualize this:
```{r}
plot(Crabs2$CW, Crabs2$SpermTotal, col=11.5*Crabs2$Color, pch=(14+Crabs2$Color), xlab = "CW", ylab="SpermTotal")
mod <- lm(SpermTotal ~ CW + factor(Color), data=Crabs2)
b0hat <- coefficients(mod)[1]
b1hat <- coefficients(mod)[2]
b2hat <- coefficients(mod)[3]
b3hat <- coefficients(mod)[4]
abline(b0hat,b1hat, col = 11.5, lwd=4)
abline(b0hat+b2hat,b1hat, col = 23, lwd=4)
abline(b0hat+b3hat, b1hat, col =34.5, lwd=4)
legend(x=12,y=25,c("Color1","Color2", "Color3"),cex=.8,col=c(11.5,23,34.5),pch=c(15,16,17))
```

b. Here we can compare three different models:

-   the first that only uses the carapace width of the crabs to explain the $SpermTotal$ variable;
-   the second that takes into account a possible effect of the color (hence the age) of the crabs in order to explain the response variable, considering three different groups and regression lines that differs only for their intercept, as we have already seen;
-   the third that considers, by adding the interaction term, that the effect of the variable $CW$ is not the same for the different Color categories. In this way the three regression lines have each a different intercept and a different slope. We can show this in the following plot: 

```{r}
mod1 <- lm(SpermTotal ~ CW + factor(Color) + CW:factor(Color), data=Crabs2)
b0tilde <- coefficients(mod1)[1]
b1tilde <- coefficients(mod1)[2]
b2tilde <- coefficients(mod1)[3]
b3tilde <- coefficients(mod1)[4]
b4tilde <- coefficients(mod1)[5]
b5tilde <- coefficients(mod1)[6]
plot(Crabs2$CW, Crabs2$SpermTotal, col=11.5*Crabs2$Color, pch=(14+Crabs2$Color), xlab = "CW", ylab="SpermTotal")
abline(b0tilde,b1tilde, col = 11.5, lwd=4)
abline(b0tilde+b2tilde,b1tilde+b4tilde, col = 23, lwd=4)
abline(b0tilde+b3tilde, b1tilde+b5tilde, col =34.5, lwd=4)
legend(x=12,y=25,c("Color1","Color2", "Color3"),cex=.8,col=c(11.5,23,34.5),pch=c(15,16,17))
```

Evaluating the results of the table we can affirm, looking at the $p-$values, that:

- the overall model is better than a null model, having it a very small $p-$value;
- the factor(Color) has a significant effect, indeed the $p-$value is sufficiently small to suggest us to accept the model with this factor;
- the interaction model does not add significant information, because $p\simeq0.22$ is large enough to allow us to ignore it and keep a simpler model.

### Ex 6.30

*When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat{\beta}_1$ in the bivariate linear model are also then multiplied by $c$. Thus, show that $r =\hat{\beta}_1(s_x/s_y)$ does not depend on the units of measurement.*

**Solution**

For simplicity we can consider two separate cases, for $c>0$ and $c <0$. Let's first consider the case $c>0$.
Let's call $z_i = c y_i$ for the sake of clarity.
We can, then calculate $s_z = s_{cy}$
\[
\begin{split}
s_z &= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (z_i - \bar z)^2}= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (cy_i - \bar {cy_i})^2}=\\
&= \sqrt{\frac{1}{n-1} \sum_{i=1}^n (cy_i - c\bar {y_i})^2}= \sqrt{\frac{c^2}{n-1} \sum_{i=1}^n (y_i - \bar {y_i})^2}=\\
&= c\sqrt{\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar {y_i})^2}= c s_y
\end{split}
\]

Let's now consider $\hat{\beta_1}'$ as the *slope* coefficient related to the response variables $z_i$.
\[
\begin{split}
\hat{\beta_1}' &= \frac{\sum_i (x_i -\bar x)(z_i -\bar z)}{\sum_i(x_i-\hat x)^2} = \frac{\sum_i (x_i -\bar x)(cy_i -\bar {cy_i})}{\sum_i(x_i-\hat x)^2} =\\
&= c \frac{\sum_i (x_i -\bar x)(y_i -\bar {y_i})}{\sum_i(x_i-\hat x)^2} = c \hat{\beta_1}
\end{split}
\]

Given these results, we can state that the quantity $r=\hat{\beta_1}\left(\frac{s_x}{s_y}\right)$ doesn't depend on the units of measurement, as
\[
r_z = \hat{\beta_1}' \frac{s_x}{s_z} = c \hat{\beta_1} \frac{s_x}{c s_y} = \hat{\beta_1} \frac{s_x}{s_y} = r_y
\]

Now, if $c<0$, we obtain, doing the same calculations, that $$s_z = |c| s_y$$, and, at the same time $$\hat\beta_1' = c \hat\beta_1$$.

So, When $c<0$, both $s_y$ and $\hat\beta_1$ preserve their magnitude, but the negative sign of $c$ leads to a negative scaling of the slope coefficient.
Despite this, the coefficient $r=\hat\beta_1 \left(\frac{s_x}{s_y}\right)$ remains unchanged.

### Ex 6.42

*You can fit the quadratic equation $\text{E}(Y)=\beta_0+\beta_1x+\beta_2x^2$ by fitting a multiple regression model with $x_1=x$ and $x_2=x^2$*.

(a) *Simulate 100 independent observations from the model $Y=40.0-5.0x+0.5x^2+\epsilon$, where $X$ has a uniform distribution over $[0,10]$ and $\epsilon\sim N(0,1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.*

(b) *Find the correlation between $x$ and $y$ and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.*

**Solution**

```{r simulate}
# simulate data
X <- runif(100, min = 0, max = 10)
e <- rnorm(100, mean = 0, sd = 1)
Y <- 40.0 - 5.0*X + 0.5*X^2 + e
plot(X, Y, main = "Simulated Data", xlab = "X", ylab = "Y")
curve(40.0 - 5.0*x + 0.5*x^2, add = TRUE, col = "red")

```
```{r model}
model <- lm(Y ~ poly(X, 2, raw = TRUE))
summary(model)
```


```{r model plot}
plot(X, model$fitted.values, main = "Fitted Values", xlab = "X", ylab = "Fitted Y")
curve(40.0 - 5.0*x + 0.5*x^2, add = TRUE, col = "red")
legend("top", legend = c("True Function", "Fitted Regression Function"), col = c("red", "black"), lty = c(1, 1), lwd = c(1, 1))
```

 b)
```{r correlation}
# correalation between x and y
cor(X, Y)
```

The correlation is weak because the relationship between $x$ and $y$ is not linear, but quadratic. The $R^2$ value is large because the quadratic model fits the data well.

### Ex 6.52

*$F$ statistics have alternate expressions in terms of $R^2$ values.*

(a)   *Show that for testing $H_0:\,\beta_1=\dots=\beta_p=0$,*$$F=\frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} \,\text{ is equivalently }\,\frac{R^2/p}{(1-R^2)/[n-(p+1)]}$$ *Explain why larger values of $R^2$ yield larger values of $F$.*

(b) *Show that for comparing nested linear models,* $$ F=\frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} =\frac{(R_1^2-R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]}$$

**Solution**

(a) Starting form the definition of the $R^2$ coefficient:

$$
R^2 = \frac{SSR}{TSS} = 1 - \frac{SSE}{TSS}
$$
We can show that by substituting it into the second expression it is possible to obtain the first one.

$$
\begin{aligned}
F &= \frac{R^2 / p}{(1-R^2) / [n-(p-1)]} = \frac{[n-(p-1)]}{p} \cdot \frac{R^2}{(1-R^2)} = \\
&= \frac{[n-(p-1)]}{p} \cdot \left( 1 - \frac{SSE}{TSS} \right) \cdot \frac{1}{1 - 1 + \frac{SSE}{TSS}} = \\
&= \frac{[n-(p-1)]}{p} \cdot \frac{TSS - SSE}{TSS} \cdot \frac{TSS}{SSE} = \\
&= \frac{[n-(p-1)]}{p} \cdot \frac{TSS - SSE}{SSE} = \\
&= \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]}
\end{aligned}
$$

which is exactly the initial expression as we expected.

The $R^2$ coefficient varies in the range $[0,1]$ and it's connected to the variability explained by the model. Values of the $R^2$ closer to one suggest that the model greatly fit the data, while lower values mean that a give model does not fit the data well enough. The F-statistics, on the other hand, is defined as the ratio between the explained variability of the data, represented by the $R^2$ itself, and the unexplained variability, given by $1-R^2$. This means that in a F-test, the greater the value of $F$, the better the model fits the data. In other words, if we find big values for the $R^2$, then the corresponding F statistic should be large since there should be strong evidence that at least some of the coefficients used in the model are significantly different than zero.

(b) Using the same procedure we can verify that the 2 expression are equivalent even in this case. By definition we know that:

$$
R^2_0 = \frac{SSR_0}{TSS} = 1 - \frac{SSE_0}{TSS}
\quad{\;\;\;}\quad
R^2_1 = \frac{SSR_1}{TSS} = 1 - \frac{SSE_1}{TSS}
$$

Note that $TSS = \sum_{i=1}^n(y_i-\bar{y})^2$ is the same for both models, since by definition it does not depend on the chosen model.\
Substituting these definition in the second expression we find that:

$$
\begin{aligned}
F &= \frac{(R_1^2-R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]} = \frac{[n-(p_1+1)}{p_1-p_0} \cdot \frac{R_1^2-R_0^2}{(1-R_1^2)} = \\
&= \frac{[n-(p_1+1)]}{p_1-p_0} \cdot \left( 1- \frac{SSE_1}{TSS} -1 + \frac{SSE_0}{TSS} \right) \cdot \left( \frac{1}{1 - 1 + \frac{SSE_1}{TSS}} \right) = \\
&= \frac{[n-(p_1+1)]}{p_1-p_0} \cdot \frac{SSE_0 - SSE_1}{TSS} \cdot \frac{TSS}{SSE_1} = \\
&= \frac{[n-(p_1+1)]}{p_1-p_0} \cdot \frac{SSE_0 - SSE_1}{SSE_1} = \\
&=\frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]}
\end{aligned}
$$

which is exactly the first expression as we expected.

## FSDS - Chapter 7

### Ex 7.4

*Analogously to the previous exercise, randomly sample 30 X observations from a uniform in the interval $(-4,4)$ and conditional on $X= x$, 30 normal observations with $\text{E}(Y)=3.5x^3-20x^2+0.5x+20$ and $\sigma = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $\text{E}(Y)=0.5x^3- 20x^2+0.5x+20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?*

**Solution**

First let's generate the random sample of observations, the gaussian errors as stochastic part of the response variable, gaussian because of dealing with a polynomial normal, and the response variables:
```{r}
X <- runif(30, min=-4, max=4)
Y <- 3.5*X^3-20*X^2+0.5*X+20
e <- rnorm(30, mean = 0, sd=30)
y <- Y + e
```
Then the two models are:
```{r}
mod2 <- glm(y ~ X + I(X^2), family = gaussian)
mod4 <- glm(y ~ X + I(X^2)+I(X^3)+I(X^4), family = gaussian)
```
We now can use and discuss two different assessing methods, AIC and ANOVA. Both will lead to the same model choice, in fact:
```{r}
anova(mod4)
cat("\n AIC for lower order model: ", AIC(mod2), "\n AIC for higher order model: ", AIC(mod4))
```
From the anova analysis we can observe a huge drop of the residuals deviance when considering the higher order model `I(x^4)`, this should mean that this model better explains the data. Of course we see that the real drop is between the second and the third order, but in the case we need to decide between the second and the fourth then the fourth order polynomial model is better. The AIC comparison shows itself that the higher order model should be better because it's AIC is smaller.

The things change in the second case. In fact, as request, if we try several times to build the model every time comparing the AIC, we see that the difference between the two values is pretty small, sometimes the simpler model AIC is even better. When in this case, the higher order is not able to better explain the data, then we should prefer the simpler quadratic model against the higher order one. In the following code we reproduced ten times the task requested, every time changing seed to evaluate the differences:
```{r}
for(i in 1:10){
  set.seed(1233+i)
  X <- runif(30, min=-4, max=4)
  Y1 <- 0.5*X^3-20*X^2+0.5*X+20
  e <- rnorm(30, mean = 0, sd=30)
  y1 <- Y1 + e
  mod2_1 <- glm(y1 ~ X + I(X^2), family = gaussian)
  mod4_1 <- glm(y1 ~ X + I(X^2)+I(X^3)+I(X^4), family = gaussian)
  cat("AIC for lower order model: ", AIC(mod2_1), " - AIC for higher order model: ", AIC(mod4_1), "\n")
}
```
Let's evaluate (only in one case for simplicity and clearness) the anova table for this case:
```{r}
anova(mod4_1)
```
We see that the residual deviance does not significantly change when increasing the model parameters, hence a simpler model should be enough.

### Ex 7.20

*In the `Crabs` data file introduced in Section 7.4.2, the variable $y$ indicates whether a female horseshoe crab has at least one satellite (1= yes, 0= no).*

(a) *Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.*

(b) *Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.*

(c) *Use AIC to determine which models seem most sensible among the models with*

    (i) interaction,

    (ii) main effects,

    (iii) weight as the sole predictor,

    (iv) color as the sole predictor,

    (v) the null model.

**Solution**

(a)

```{r}
crabs = read.table("data/Crabs.dat", header=T)
main_model = glm(y ~ weight + factor(color), data=crabs, family=binomial)
summary(main_model)
```
In this case we can say that none of the colors in the dataset show a statistical significance at a typical significance level ($\alpha = 0.05$). In fact, as their p-value is greater than this significance level, this shows weak evidence against the null hypothesis of $\hat\beta_j = 0$. So, we can say that there is insufficient evidence to conclude that any level of the covariate *color* affects the response variable $y$.

```{r}
# Confidence interval for weight effect
CI<-coef(main_model)[2] +c(-1,1) * summary(main_model)$coefficients[2,2] * qnorm(0.975)
cat("Confidence Interval for the effect of weight: (", CI[1], ",", CI[2], ")\n")
```

(b)
```{r}
# Fit the model with interaction
interaction_model <- glm(y ~ weight * factor(color), data = crabs, family = binomial)
summary(interaction_model)

# Compare models using likelihood ratio test
anova(main_model, interaction_model, test = "Chisq")

```
As we can see, although looking at the only AIC value would suggest taking the model with interaction as a better one, looking at the p-value of the chi-squared statistics calculated upon the deviances, we can see that we don't have a significant improvement in the model, even considering the interaction factor. This suggests us to stick with the simpler model without interaction.

(c)
```{r}
#Model with interaction
interaction_model <- glm(y ~ weight * factor(color), data = crabs, family = binomial)
AIC_interaction <- AIC(interaction_model)

#Model without interaction
main_model <- glm(y ~ weight + factor(color), data = crabs, family = binomial)
AIC_main <- AIC(main_model)

#Model with only weight
weight_model <- glm(y ~ weight, data = crabs, family = binomial)
AIC_weight <- AIC(weight_model)

#Model with only color
color_model <- glm(y ~ factor(color), data = crabs, family = binomial)
AIC_color <- AIC(color_model)

#Null model
null_model <- glm(y ~ 1, data = crabs, family = binomial)
AIC_null <- AIC(null_model)

#AIC comperison
AIC_values <- c("Interaction" = AIC_interaction, 
                "Main Effects" = AIC_main, 
                "Weight Only" = AIC_weight, 
                "Color Only" = AIC_color, 
                "Null Model" = AIC_null)

AIC_values

```
Looking at only the AIC value, the model that seems best fit on the data is the one considering the interaction between *weight* and *color* covariates. So it seems to be the best balance between goodness of fit and complexity.

The second one in this list is the one that considers the simple interaction between *weight* and *color*, that has a slightly lower AIC index. This suggests that considering *weight* and *color* ad main effect without considering their interaction provides a reasonably good fit, too, while being less complex.

Also the model considering the *wieght* parameter as only covariate seems to have the same explanation.

The remaining two models have higher AIC, and considering they are simpler models, they suggest poorer fit of the data.

So considering the AIC index, the most sensible choice appears to be the one including the interaction between covariates, closely followed by the model without interaction.

### Ex 7.26

*A headline in ``The Gainesville Sun (Feb. 17, 2014)`` proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.*

**Solution**

```{r shark}

shark_attacks <- c(33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23)

# Plot she shark attacks
plot(shark_attacks, main = "Shark Attacks", xlab = "Year", ylab = "Number of Attacks")



```

```{r mean and variance}
# Calculate the mean
mean_attacks <- mean(shark_attacks)

# Calculate the variance
var_attacks <- var(shark_attacks)

# Print mean and variance
cat("Mean:", mean_attacks, "\n")
cat("Variance:", var_attacks, "\n")
```

As we can see, the variance is greater than the mean, so the data are overdispersed. Therefore we can't use a Poisson model. We can use a negative binomial model instead.

```{r negative binomial}
#import the glm.nb function
library(MASS)

# Fit a negative binomial model
model <- glm.nb(shark_attacks ~ 1)

# Print the summary
summary(model)
```

To have a comparison, let's fit a Poisson model now.

```{r poisson}
# Fit a Poisson model
model_poisson <- glm(shark_attacks ~ 1, family = poisson)

# Print the summary
summary(model_poisson)

```

As we can see, the estimated values of the intercept are the same, but the negative binomial model has a lower AIC and lower deviance, so it is a better fit. This could be motivated by the fact that the negative binomial model allows for overdispersion, which is present in the data.

## DAAG - Chapter 8

### Ex 6

*As in the previous exercise, the function ``poissonsim()`` allows for experimentation with
Poisson regression. In particular, ``poissonsim()`` can be used to simulate Poisson responses
with log-rates equal to a + bx, where a and b are fixed values by default.\
(a) Simulate 100 Poisson responses using the model $$log \lambda = 2 − 4x$$\
for x = 0, 0.01, 0.02 ..., 1.0. Fit a Poisson regression model to these data, and compare the
estimated coefficients with the true coefficients. How well does the estimated model predict
future observations?\
(b) Simulate 100 Poisson responses using the model $$log \lambda = 2 − bx$$
where b is normally distributed with mean 4 and standard deviation 5. [Use the argument
slope.sd=5 in the `poissonsim()`` function] How do the results using the poisson
and ``quasipoisson`` families differ?*


**Solution**

(a) We can first simulate a sequence of 100 Poisson responses that respect the given model using the ``poissonsim()`` function in this way:

```{r}
library(DAAG)
x_sim <- seq(0.0,1.0,0.01)
data_sim <- poissonsim(x_sim, 2, -4) 
```

The, for the sake of clarity, if we label with $x$ the covariates and with $y$ the responses, we can perform a fit using the ``glm()`` command with the Poisson family.

```{r}
x <- data_sim[,1]
y <- data_sim[,2]

pois_fit <- glm(y~x, family = poisson(link=log))
summary(pois_fit)
```

We can therefore see that the value obtained for the estimated parameters is very close to the actual ones, with a small standard error. The small p-values obtained for both the parameters also allow us to reject the hypothesis that the parameters are equal to zero.

*...I should add some comments on the deviances and AIC which is very high...*

(b) If the parameter b is now normally distributed with mean 4 and standard deviation 5 we can simulate the data in a similar way using the suggested argument for the ``poissonsim()`` function.

```{r}
x_sim <- seq(0.0,1.0,0.01)
data_sim <- poissonsim(x_sim, 2, -4, slope.sd=5) 
```

Just as done before we can assign better names to the variables and fit the data first using the poisson family and then the quasipoisson:

```{r}
x <- data_sim[,1]
y <- data_sim[,2]
pois_fit <- glm(y~x, family = poisson(link=log))
quasipois_fit <- glm(y~x, family = quasipoisson(link=log))

summary(pois_fit)
```
```{r}
summary(quasipois_fit)
```

We can see that for both families the estimated values for the parameters are exactly the same just as we would expect since the estimating equations do not change from one to the other. However, the standard error for the parameters is different since in the quasipoisson method, in order to deal with the overdispersion of the data, the variance of the response variable is modeled as $var(Y_i) = \phi var(\mu_i)$ and a value different from $1$ is estimated for $\phi$ unlike in the first case.


















